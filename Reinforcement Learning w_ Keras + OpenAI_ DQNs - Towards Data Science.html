<!DOCTYPE html>
<!-- saved from url=(0085)https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/branch-latest.min.js"></script><script async="" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/analytics.js"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>Reinforcement Learning w/ Keras + OpenAI: DQNs - Towards Data Science</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2017-07-30T08:22:13.411Z"><meta data-rh="true" name="title" content="Reinforcement Learning w/ Keras + OpenAI: DQNs - Towards Data Science"><meta data-rh="true" property="og:title" content="Reinforcement Learning w/ Keras + OpenAI: DQNs"><meta data-rh="true" property="twitter:title" content="Reinforcement Learning w/ Keras + OpenAI: DQNs"><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/1eed3a5338c"><meta data-rh="true" property="al:android:url" content="medium://p/1eed3a5338c"><meta data-rh="true" property="al:ios:url" content="medium://p/1eed3a5338c"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="Last time in our Keras/OpenAI tutorial, we discussed a very basic example of applying deep learning to reinforcement learning contexts. This was an incredible showing in retrospect! If you looked at…"><meta data-rh="true" property="og:description" content="Quick Recap"><meta data-rh="true" property="twitter:description" content="Quick Recap"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c"><meta data-rh="true" property="og:image" content="https://miro.medium.com/freeze/max/598/1*nbCSvWmyS_BUDz_WAJyKUw.gif"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/freeze/max/598/1*nbCSvWmyS_BUDz_WAJyKUw.gif"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://towardsdatascience.com/@yashpatel_86510"><meta data-rh="true" name="author" content="Yash Patel"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="11 min read"><meta data-rh="true" name="parsely-post-id" content="1eed3a5338c"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/m2.css"><link data-rh="true" rel="author" href="https://towardsdatascience.com/@yashpatel_86510"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/1eed3a5338c"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="410" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:fixed}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ai{max-width:1192px}.aj{min-width:0}.ak{width:100%}.al{height:65px}.ao{flex:1 0 auto}.ap{flex:0 0 auto}.aq{visibility:hidden}.ar{margin-left:16px}.as{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.at{font-style:normal}.au{line-height:20px}.av{font-size:15.8px}.aw{letter-spacing:0px}.ax{color:rgba(0, 0, 0, 0.54)}.ay{fill:rgba(0, 0, 0, 0.54)}.az{color:rgba(90, 118, 144, 1)}.ba{fill:rgba(102, 138, 170, 1)}.bb{font-size:inherit}.bc{border:inherit}.bd{font-family:inherit}.be{letter-spacing:inherit}.bf{font-weight:inherit}.bg{padding:0}.bh{margin:0}.bi:hover{cursor:pointer}.bj:hover{color:rgba(84, 108, 131, 1)}.bk:hover{fill:rgba(90, 118, 144, 1)}.bl:focus{outline:none}.bm:disabled{cursor:default}.bn:disabled{color:rgba(3, 168, 124, 0.5)}.bo:disabled{fill:rgba(3, 168, 124, 0.5)}.bp{padding:8px 16px}.bq{background:0}.br{border-color:rgba(102, 138, 170, 1)}.bs:hover{border-color:rgba(90, 118, 144, 1)}.bt{border-radius:4px}.bu{border-width:1px}.bv{border-style:solid}.bw{box-sizing:border-box}.bx{display:inline-block}.by{text-decoration:none}.bz{border-top:none}.ca{background-color:rgba(53, 88, 118, 1)}.cc{height:54px}.cd{overflow:hidden}.ce{margin-right:40px}.cf{height:36px}.cg{width:100px}.ch{overflow:auto}.ci{flex:0 1 auto}.cj{list-style-type:none}.ck{line-height:40px}.cl{white-space:nowrap}.cm{overflow-x:auto}.cn{align-items:flex-start}.co{margin-top:20px}.cp{padding-top:20px}.cq{height:80px}.cr{height:20px}.cs{margin-right:15px}.ct{margin-left:15px}.cu:first-child{margin-left:0}.cv{min-width:1px}.cw{background-color:rgba(197, 210, 225, 1)}.cx{font-weight:300}.cy{font-size:15px}.cz{color:rgba(197, 210, 225, 1)}.da{text-transform:uppercase}.db{letter-spacing:1px}.dc{color:inherit}.dd{fill:inherit}.de:hover{color:rgba(251, 255, 255, 1)}.df:hover{fill:rgba(233, 241, 250, 1)}.dg:disabled{color:rgba(150, 171, 191, 1)}.dh:disabled{fill:rgba(150, 171, 191, 1)}.di{margin-bottom:0px}.dj{height:119px}.dm{padding-left:24px}.dn{padding-right:24px}.do{margin-left:auto}.dp{margin-right:auto}.dq{max-width:728px}.dr{flex-direction:column}.ds{position:absolute}.dt{top:calc(100vh + 100px)}.du{bottom:calc(100vh + 100px)}.dv{width:10px}.dw{pointer-events:none}.dx{word-break:break-word}.dy{word-wrap:break-word}.dz:after{display:block}.ea:after{content:""}.eb:after{clear:both}.ec{max-width:680px}.ed{line-height:1.23}.ee{letter-spacing:0}.ef{color:rgba(0, 0, 0, 0.84)}.eg{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.er{margin-bottom:-0.27em}.ex{margin-top:32px}.ey{justify-content:space-between}.fc{border-radius:50%}.fd{height:48px}.fe{width:48px}.ff{margin-left:12px}.fg{margin-bottom:2px}.fi{font-size:16px}.fj{max-height:20px}.fk{text-overflow:ellipsis}.fl{display:-webkit-box}.fm{-webkit-line-clamp:1}.fn{-webkit-box-orient:vertical}.fo:hover{text-decoration:underline}.fp:disabled{color:rgba(0, 0, 0, 0.54)}.fq:disabled{fill:rgba(0, 0, 0, 0.54)}.fr{margin-left:8px}.fs{padding:0px 8px}.ft{border-color:rgba(0, 0, 0, 0.54)}.fu:hover{color:rgba(0, 0, 0, 0.97)}.fv:hover{fill:rgba(0, 0, 0, 0.97)}.fw:hover{border-color:rgba(0, 0, 0, 0.84)}.fx:disabled{fill:rgba(0, 0, 0, 0.76)}.fy:disabled{border-color:rgba(0, 0, 0, 0.2)}.fz:disabled{cursor:inherit}.ga:disabled:hover{color:rgba(0, 0, 0, 0.54)}.gb:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.gc:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.gd{line-height:18px}.ge{align-items:flex-end}.gm{padding-right:8px}.gn:hover{color:rgba(0, 0, 0, 0.9)}.go:hover{fill:rgba(0, 0, 0, 0.9)}.gp{margin-right:-6px}.gq{line-height:1.58}.gr{letter-spacing:-0.004em}.gs{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.hd{margin-bottom:-0.46em}.he{font-weight:700}.hf{max-width:598px}.hl{clear:both}.hm{opacity:0}.hn{transition:opacity 100ms 400ms}.ho{height:100%}.hp{will-change:transform}.hq{transform:translateZ(0)}.hr{margin:auto}.hs{position:relative}.ht{background-color:rgba(0, 0, 0, 0.05)}.hu{padding-bottom:66.22073578595318%}.hv{filter:blur(20px)}.hw{transform:scale(1.1)}.hx{visibility:visible}.hy{background:rgba(255, 255, 255, 1)}.hz{line-height:1.4}.ia{margin-top:10px}.ib{text-align:center}.ie{max-width:686px}.if{border-width:2px}.ig{border-color:rgba(255, 255, 255, 1)}.ih{float:left}.ii{margin-left:-150px}.ij{margin-right:30px}.ik{width:75%}.il{padding-bottom:10px}.iq{margin-bottom:16px}.ir{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.is{cursor:zoom-in}.it{z-index:auto}.iu{padding-bottom:32.6530612244898%}.iv{font-style:italic}.iw{max-width:740px}.ix{padding-bottom:30.54054054054054%}.iy{padding:20px}.iz{background:rgba(0, 0, 0, 0.05)}.ja{line-height:1.18}.jb{letter-spacing:-0.022em}.jc{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.jd{margin-top:-0.09em}.je{margin-bottom:-0.09em}.jf{white-space:pre-wrap}.jl{padding-bottom:NaN%}.jm{line-height:1.12}.jn{font-weight:600}.jw{margin-bottom:-0.28em}.jx{will-change:opacity}.jy{width:188px}.jz{left:50%}.ka{transform:translateX(406px)}.kb{top:calc(65px + 54px + 14px)}.ke{top:calc(65px + 54px + 40px)}.kg{width:131px}.kh{padding-bottom:28px}.ki{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.kj{font-size:18px}.kk{padding-bottom:20px}.kl{padding-top:2px}.km{max-height:120px}.kn{-webkit-line-clamp:6}.ko{padding:4px 12px}.kp{padding-top:28px}.kq{margin-bottom:19px}.kr{margin-left:-5px}.ks{margin-right:5px}.kt{outline:0}.ku{border:0}.kv{user-select:none}.kw{cursor:pointer}.kx> svg{pointer-events:none}.ky:active{border-style:none}.kz{-webkit-user-select:none}.la:focus{fill:rgba(90, 118, 144, 1)}.lb{margin-top:5px}.lc button{text-align:left}.ld{margin-top:40px}.le{flex-wrap:wrap}.lf{margin-top:25px}.lg{margin-right:8px}.lh{margin-bottom:8px}.li{border-radius:3px}.lj{padding:5px 10px}.lk{line-height:22px}.ll{margin-top:15px}.lm{margin-right:16px}.ln{border:1px solid rgba(0, 0, 0, 0.1)}.lo{height:60px}.lp{width:60px}.mc:active{border-style:solid}.md{z-index:2}.mf{padding-top:32px}.mg{border-top:1px solid rgba(0, 0, 0, 0.1)}.mh{margin-bottom:25px}.mi{margin-bottom:32px}.mj{min-height:80px}.mo{width:80px}.mp{padding-left:102px}.mr{letter-spacing:0.05em}.ms{margin-bottom:6px}.mt{font-size:28px}.mu{line-height:36px}.mv{max-width:555px}.mw{max-width:450px}.mx{line-height:24px}.my{display:none}.na{max-width:550px}.nb{padding-top:25px}.nc{color:rgba(0, 0, 0, 0.76)}.nd{opacity:1}.ne{border:1px solid rgba(102, 138, 170, 1)}.nf{margin-top:64px}.ng{background-color:rgba(0, 0, 0, 0.02)}.nh{padding:60px 0}.ni{background-color:rgba(0, 0, 0, 0.9)}.nz{padding-bottom:48px}.oa{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.ob{margin:0 -12px}.oc{margin:0 12px}.od{flex:1 1 0}.oe{padding-bottom:12px}.of:hover{color:rgba(255, 255, 255, 0.99)}.og:hover{fill:rgba(255, 255, 255, 0.99)}.oh:disabled{color:rgba(255, 255, 255, 0.7)}.oi:disabled{fill:rgba(255, 255, 255, 0.7)}.oj{color:rgba(255, 255, 255, 0.98)}.ok{fill:rgba(255, 255, 255, 0.98)}.ol{text-align:inherit}.om{font-size:21.6px}.on{letter-spacing:-0.32px}.oo{color:rgba(255, 255, 255, 0.7)}.op{fill:rgba(255, 255, 255, 0.7)}.oq{text-decoration:underline}.or{padding-bottom:8px}.os{padding-top:8px}.ot{width:200px}.ov{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.ah{margin:0 64px}.ep{font-size:40px}.eq{margin-top:0.78em}.ew{line-height:48px}.gl{margin-left:30px}.hb{font-size:21px}.hc{margin-top:2em}.hk{margin-top:56px}.jk{margin-top:1.91em}.ju{font-size:34px}.jv{margin-top:1.95em}.nw{padding-left:64px}.nx{padding-right:64px}.ny{max-width:1320px}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.gk{margin-left:30px}.ic{margin-left:auto}.id{text-align:center}.im{float:none}.in{margin-left:0}.io{margin-right:0}.ip{width:100%}.nt{padding-left:64px}.nu{padding-right:64px}.nv{max-width:1080px}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.gj{margin-left:30px}.nq{padding-left:48px}.nr{padding-right:48px}.ns{max-width:904px}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.am{height:56px}.an{display:flex}.cb{display:block}.dk{margin-bottom:0px}.dl{height:110px}.fa{margin-top:32px}.fb{flex-direction:column-reverse}.gh{margin-bottom:30px}.gi{margin-left:0px}.mk{margin-bottom:24px}.ml{align-items:center}.mm{width:102px}.mn{position:relative}.mq{padding-left:0}.mz{margin-top:24px}.nj{padding:32px 0}.nn{padding-left:24px}.no{padding-right:24px}.np{max-width:728px}.ou{width:140px}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ac{margin:0 24px}.eh{font-size:30px}.ei{margin-top:0.72em}.es{line-height:40px}.ez{margin-top:32px}.fh{margin-bottom:0px}.gf{margin-bottom:30px}.gg{margin-left:0px}.gt{font-size:18px}.gu{margin-top:1.56em}.hg{margin-top:40px}.jg{margin-top:1.41em}.jo{margin-top:1.2em}.nk{padding-left:24px}.nl{padding-right:24px}.nm{max-width:552px}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ag{margin:0 64px}.en{font-size:40px}.eo{margin-top:0.78em}.ev{line-height:48px}.gz{font-size:21px}.ha{margin-top:2em}.hj{margin-top:56px}.jj{margin-top:1.91em}.js{font-size:34px}.jt{margin-top:1.95em}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.af{margin:0 48px}.el{font-size:40px}.em{margin-top:0.78em}.eu{line-height:48px}.gx{font-size:21px}.gy{margin-top:2em}.hi{margin-top:56px}.ji{margin-top:1.91em}.jq{font-size:34px}.jr{margin-top:1.95em}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ae{margin:0 24px}.ej{font-size:30px}.ek{margin-top:0.72em}.et{line-height:40px}.gv{font-size:18px}.gw{margin-top:1.56em}.hh{margin-top:40px}.jh{margin-top:1.41em}.jp{margin-top:1.2em}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="print">.ab{display:none}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.y{transition:transform 300ms ease}.z{will-change:transform}.kc{transition:opacity 200ms}.lq{transition:border-color 150ms ease}.lr::before{background:
      radial-gradient(circle, rgba(90, 118, 144, 1) 60%, transparent 70%)
    }.ls::before{border-radius:50%}.lt::before{content:""}.lu::before{display:block}.lv::before{z-index:0}.lw::before{left:0}.lx::before{height:100%}.ly::before{position:absolute}.lz::before{top:0}.ma::before{width:100%}.mb:hover::before{animation:k1 2000ms infinite cubic-bezier(.1,.12,.25,1)}.me{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (max-width: 1230px)">.kd{display:none}</style><style type="text/css" data-fela-rehydration="410" data-fela-type="RULE" media="all and (max-width: 1198px)">.kf{display:none}</style><script charset="utf-8" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/vendors_tracing.6badc2b7.chunk.js"></script><script charset="utf-8" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/tracing.61367508.chunk.js"></script><link rel="icon" href="https://miro.medium.com/fit/c/128/128/1*ChFMdf--f5jbm-AYv6VdYA@2x.png" data-rh="true"><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Ffreeze\u002Fmax\u002F1200\u002F1*nbCSvWmyS_BUDz_WAJyKUw.gif"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c","dateCreated":"2017-07-30T07:02:37.835Z","datePublished":"2017-07-30T07:02:37.835Z","dateModified":"2018-06-21T08:53:33.888Z","headline":"Reinforcement Learning w\u002F Keras + OpenAI: DQNs","name":"Reinforcement Learning w\u002F Keras + OpenAI: DQNs","description":"Last time in our Keras\u002FOpenAI tutorial, we discussed a very basic example of applying deep learning to reinforcement learning contexts. This was an incredible showing in retrospect! If you looked at…","identifier":"1eed3a5338c","keywords":["Lite:true","Tag:Machine Learning","Tag:Keras","Tag:Deep Leraning","Tag:Reinforcement Learning","Tag:Towards Data Science","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:0"],"author":{"@type":"Person","name":"Yash Patel","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@yashpatel_86510"},"creator":["Yash Patel"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F165\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y z ab" style="transform: translateY(-100%);"><div class="branch-journeys-top"><div class="r c"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="al n o am an"><div class="n o ao w"><a href="https://medium.com/?source=post_page-----1eed3a5338c----------------------" aria-label="Homepage" rel="noopener"><svg width="35" height="35" viewBox="5 5 35 35" class="q"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a></div><div class="r ap w"><div class="n o"><div class="n g"><div class="hx" id="lo-post-page-navbar-sign-in-link"><div class="ar r"><span class="as b at au av aw r ax ay"><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=post_page-----1eed3a5338c---------------------nav_reg-" class="az ba bb bc bd be bf bg bh bi bj bk bl bm bn bo" rel="noopener">Sign in</a></span></div></div></div><div class="hx" id="lo-post-page-navbar-get-started-button"><div class="ar r"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=post_page-----1eed3a5338c---------------------nav_reg-" class="bp bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl" rel="noopener">Get started</a></div></div></div></div></div></div></div></div><div class="bz r ca cb"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="cc cd n o"><div class="ce r ap"><a href="https://towardsdatascience.com/?source=post_page-----1eed3a5338c----------------------" rel="noopener"><div class="cf cg r"><img alt="Towards Data Science" class="" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1_mG6i4Bh_LgixUYXJgQpYsg@2x.png" width="100" height="36"></div></a></div><div class="ch r ci"><ul class="cj bh ck cl cm n cn g co cp cq"><li class="n o cr cs ct cu"><span class="as cx cy au cz da db"><a href="https://towardsdatascience.com/data-science/home?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi de df bl bm dg dh" rel="noopener">Data Science</a></span></li><li class="n o cr cs ct cu"><span class="as cx cy au cz da db"><a href="https://towardsdatascience.com/machine-learning/home?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi de df bl bm dg dh" rel="noopener">Machine Learning</a></span></li><li class="n o cr cs ct cu"><span class="as cx cy au cz da db"><a href="https://towardsdatascience.com/programming/home?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi de df bl bm dg dh" rel="noopener">Programming</a></span></li><li class="n o cr cs ct cu"><span class="as cx cy au cz da db"><a href="https://towardsdatascience.com/data-visualization/home?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi de df bl bm dg dh" rel="noopener">Visualization</a></span></li><li class="n o cr cs ct cu"><span class="as cx cy au cz da db"><a href="https://towardsdatascience.com/artificial-intelligence/home?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi de df bl bm dg dh" rel="noopener">AI</a></span></li><li class="n o cr cs ct cu"><span class="as cx cy au cz da db"><a href="https://towardsdatascience.com/our-picks/home?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi de df bl bm dg dh" rel="noopener">Picks</a></span></li><li class="n o cr cs ct cu"><span class="as cx cy au cz da db"><a href="https://towardsdatascience.com/more/home?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi de df bl bm dg dh" rel="noopener">More</a></span></li><span class="cr cv cw"></span><li class="n o cr cs ct cu"><span class="as cx cy au cz da db"><a href="https://towardsdatascience.com/contribute/home?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi de df bl bm dg dh" rel="noopener">Contribute</a></span></li></ul></div></div></div></div></div></div></nav><div class="di dj r dk dl"></div><article><section class="dm dn do dp ak dq bw n dr"></section><span class="r"></span><div><div class="ds u dt du dv dw"></div><div class="do dp dq hs"><div class="r h g f e"><aside class="qh ds t" style="width: 571.5px;"><div class="qk ql ds qm cl ak"><h4 class="as cx cy au ax"><span class="bx ql cl cd fk">Top highlight</span></h4></div></aside></div></div><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><div><div id="12b9" class="ed ee ef at eg b eh ei ej ek el em en eo ep eq er"><h1 class="eg b eh es ej et el eu en ev ep ew ef">Reinforcement Learning w/ Keras + OpenAI: DQNs</h1></div><div class="ex"><div class="n ey ez fa fb"><div class="o n"><div><a href="https://towardsdatascience.com/@yashpatel_86510?source=post_page-----1eed3a5338c----------------------" rel="noopener"><img alt="Yash Patel" class="r fc fd fe" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1____MnwCLk7NXM6xZIipDBA.png" width="48" height="48"></a></div><div class="ff ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r ef q"><div class="fg n o fh"><span class="as cx fi au cd fj fk fl fm fn ef"><a href="https://towardsdatascience.com/@yashpatel_86510?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi fo bl bm fp fq" rel="noopener">Yash Patel</a></span><div class="fr r ap h"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=-68694da71a5f-------------------------follow_byline-" class="fs ef q bq ft fu fv fw bi fp fx fy fz ga gb gc bt as b at gd cy aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cx fi au cd fj fk fl fm fn ax"><div><a class="dc dd bb bc bd be bf bg bh bi fo bl bm fp fq" rel="noopener" href="https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c?source=post_page-----1eed3a5338c----------------------">Jul 30, 2017</a> <!-- -->·<!-- --> <!-- -->11<!-- --> min read</div></span></span></div></div><div class="n ge gf gg gh gi gj gk gl ab"><div class="n o"><div class="gm r ap"><a href="https://medium.com/p/1eed3a5338c/share/twitter?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gm r ap"><a href="https://medium.com/p/1eed3a5338c/share/facebook?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gp r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="dc33" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">Quick Recap</strong></p><p id="ce16" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Last time in our Keras/OpenAI tutorial, we discussed a very basic example of applying deep learning to reinforcement learning contexts. This was an incredible showing in retrospect! If you looked at the training data, the random chance models would usually only be able to perform for 60 steps in median. And yet, by training on this seemingly very mediocre data, we were able to “beat” the environment (i.e. get &gt;200 step performance). How is this possible?</p><p id="fd26" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">We can get directly an intuitive feel for this. Let’s imagine the perfectly random series we used as our training data. It is extremely unlikely that any two series will have high overlap with one another, since these are generated completely randomly. However, there <strong class="gs he">are</strong> key features that are common between successful trials, such as pushing the cart right when the pole is leaning right and vice versa. And so, by training our NN on all these trials data, we extract the shared patterns that contributed to them being successful and are able to smooth over the details that resulted in their independent failures.</p><p id="4a08" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">That being said, the environment we consider this week is significantly more difficult than that from last week: the MountainCar.</p><p id="d773" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">More Complex Environments</strong></p><p id="f202" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Even though it seems we should be able to apply the same technique as that we applied last week, there is one key features here that makes doing so impossible: we can’t generate training data. Unlike the very simple Cartpole example, taking random movements often simply leads to the trial ending in us at the bottom of the hill. That is, we have several trials that are all identically -200 in the end. This is practically useless to use as training data. Imagine you were in a class where no matter what answers you put on your exam, you got a 0%! How are you going to learn from any of those experiences?</p><figure class="hg hh hi hj hk hl do dp paragraph-image"><div class="do dp hf"><div class="hr r hs ht"><div class="hu r"><div class="hm hn ds t u ho ak cd hp hq"><img class="ds t u ho ak hv hw aq qn" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1_nbCSvWmyS_BUDz_WAJyKUw.gif" width="598" height="396" role="presentation"></div><img class="nd qf ds t u ho ak hy" width="598" height="396" role="presentation" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1_nbCSvWmyS_BUDz_WAJyKUw(1).gif"><noscript><img class="ds t u ho ak" src="https://miro.medium.com/max/1196/1*nbCSvWmyS_BUDz_WAJyKUw.gif" width="598" height="396" role="presentation"/></noscript></div></div></div><figcaption class="ax fi hz ia ib dq do dp ic id as cx" data-selectable-paragraph="">Random inputs for the “MountainCar-v0” environment does not produce any output that is worthwhile or useful to train on</figcaption></figure><p id="4849" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">In line with that, we have to figure out a way to incrementally improve upon previous trials. For this, we use one of the most basic stepping stones for reinforcement learning: Q-learning!</p><p id="46a4" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">DQN Theory Background</strong></p><p id="97ca" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Q-learning (which doesn’t stand for anything, by the way) is centered around creating a “virtual table” that accounts for how much reward is assigned to each possible action given the current state of the environment. Let’s break that down one step at a time:</p><figure class="hg hh hi hj hk hl hy if bv ig ih ii ij ik bg il im in io ip iq paragraph-image"><div class="ir is hs it ak"><div class="do dp ie"><div class="hr r hs ht"><div class="iu r"><div class="hm hn ds t u ho ak cd hp hq"><img class="ds t u ho ak hv hw aq qn" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1_WEfdub-U4i5HRLunOlXj8g.png" width="686" height="224" role="presentation"></div><img class="nd qf ds t u ho ak hy" width="686" height="224" role="presentation" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1_WEfdub-U4i5HRLunOlXj8g(1).png"><noscript><img class="ds t u ho ak" src="https://miro.medium.com/max/1372/1*WEfdub-U4i5HRLunOlXj8g.png" width="686" height="224" role="presentation"/></noscript></div></div></div></div><figcaption class="ax fi hz ia ib dq do dp ic id as cx" data-selectable-paragraph="">You can imagine the DQN network as internally maintaining a spreadsheet of the values of each of the possible actions that can be taken given the current environment state</figcaption></figure><p id="84af" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">What do we mean by “virtual table?” Imagine that for each possible configuration of the input space, you have a table that assigns a score for each of the possible actions you can take. If this were magically possible, then it would be extremely easy for you to “beat” the environment: simply choose the action that has the highest score! Two points to note about this score. First, this score is conventionally referred to as the “Q-score,” which is where the name of the overall algorithm comes from. Second, as with any other score, these Q score have <em class="iv">no meaning </em>outside the context of their simulation. That is, they have <strong class="gs he">no absolute significance</strong>, but that’s perfectly fine, since we solely need it to do comparisons.</p><p id="cfbe" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Why then do we need virtual table for <em class="iv">each </em>input configuration? Why can’t we just have one table to rule them all? The reason is that it doesn’t make sense to do so: that would be the same as saying the best action to take while at the bottom of the valley is exactly that which you should take when you are perched on the highest point of the left incline.</p><p id="984c" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Now, the main problem with what I described (maintaining a virtual table for <em class="iv">each </em>input configuration) is that this is impossible: we have a continuous (infinite) input space! We could get around this by discretizing the input space, but that seems like a pretty hacky solution to this problem that we’ll be encountering over and over in future situations. So, how do we get around this? By applying neural nets to the situation: that’s where the D in DQN comes from!</p><p id="de5c" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">DQN Agent</strong></p><p id="ef63" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">So, we’ve now reduced the problem to finding a way to assign the different actions Q-scores given the current state. This is the answer to a very natural first question to answer when employing any NN: what are the inputs and outputs of our model? The extent of the math you need to understand for this model is the following equation (don’t worry, we’ll break it down):</p><figure class="hg hh hi hj hk hl do dp paragraph-image"><div class="ir is hs it ak"><div class="do dp iw"><div class="hr r hs ht"><div class="ix r"><div class="hm hn ds t u ho ak cd hp hq"><img class="ds t u ho ak hv hw aq qn" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1_IyrdggWA1wsce4nBA6u2VA.png" width="740" height="226" role="presentation"></div><img class="nd qf ds t u ho ak hy" width="740" height="226" role="presentation" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1_IyrdggWA1wsce4nBA6u2VA(1).png"><noscript><img class="ds t u ho ak" src="https://miro.medium.com/max/1480/1*IyrdggWA1wsce4nBA6u2VA.png" width="740" height="226" role="presentation"/></noscript></div></div></div></div></figure><p id="0d55" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Q, as mentioned, represents the value estimated by our model given the current state (s) and action taken (a). The goal, however, is to determine the <em class="iv">overall </em>value of a state. What do I mean by that? The <em class="iv">overall </em>value is <strong class="gs he">both </strong>the immediate reward you will get <strong class="gs he">and </strong>the expected rewards you will get in the future from being in that position. That is, we want to account for the fact that the value of a position often reflects not only its immediate gains but also the future gains it enables (damn, deep). In any case, we discount future rewards because, if I compare two situations in which I expect to get $100 one of the two being in the future, I would always take the present deal, since the position of the future one may change between when I made the deal and when I receive the money. The gamma factor reflects this depreciated value for the expected future returns on the state.</p><p id="6ab5" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">And that’s it: that’s all the math we’ll need for this! Time to actually move on to some code!</p><p id="a607" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">DQN Agent Implementation</strong></p><p id="01cd" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><mark class="qi qj kw">The Deep Q Network revolves around continuous learning, meaning that we don’t simply accrue a bunch of trial/training data and feed it into the model. Instead, we create training data through the trials we run and feed this information into it directly after running the trial.</mark> If this all seems somewhat vague right now, don’t worry: time to see some code about this. The code largely revolves around defining a DQN class, where all the logic of the algorithm will actually be implemented, and where we expose a simple set of functions for the actual training.</p><p id="b899" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">DQN Hyperparameters</strong></p><p id="6893" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">First off, we’re going to discuss some parameters of relevance for DQNs. Most of them are standard from most neural net implementations:</p><pre class="hg hh hi hj hk iy iz cm"><span id="fbb4" class="ja jb ef at jc b fi jd je r jf" data-selectable-paragraph="">class DQN:<br>    def __init__(self, env):<br>        self.env     = env<br>        self.memory  = deque(maxlen=2000)<br>        <br>        self.gamma = 0.95<br>        self.epsilon = 1.0<br>        self.epsilon_min = 0.01<br>        self.epsilon_decay = 0.995<br>        self.learning_rate = 0.01</span></pre><p id="ab97" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Let’s step through these one at a time. The first is simply the environment, which we supply for convenience when we need to reference the shapes in creating our model. The “memory” is a key component of DQNs: as mentioned previously, the trials are used to continuously train the model. However, rather than training on the trials as they come in, we add them to memory and train on a random sample of that memory. Why do this instead of just training on the last <em class="iv">x </em>trials as our “sample?” The reason is somewhat subtle. Imagine instead we were to just train on the most recent trials as our sample: in this case, our results would only learn on its most recent actions, which may not be directly relevant for future predictions. In this environment in particular, if we were moving down the right side of the slope, training on the most recent trials would entail training on the data where you were moving up the hill towards the right. But, this would not be at all relevant to determining what actions to take in the scenario you would soon be facing of scaling up the left hill. So, by taking a random sample, we don’t bias our training set, and instead ideally learn about scaling all environments we would encounter equally well.</p><p id="b6a4" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">So, we now discuss hyperparameters of the model: gamma, epsilon/epsilon decay, and the learning rate. The first is the future rewards depreciation factor (&lt;1) discussed in the earlier equation, and the last is the standard learning rate parameter, so I won’t discuss that here. The second, however, is an interesting facet of RL that deserves a moment to discuss. In any sort of learning experience, we always have the choice between exploration vs. exploitation. This isn’t limited to computer science or academics: we do this on a day to day basis!</p><p id="2e01" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Consider the restaurants in your local neighborhood. When was the last time you went to a new one? Probably a long time ago. That corresponds to your shift from <em class="iv">exploration </em>to <em class="iv">exploitation</em>: rather than trying to find new and better opportunities, you settle with the best one you’ve found in your past experiences and maximize your utility from there. Contrast that to when you moved into your house: at that time, you had no idea what restaurants were good or not and so were enticed to explore your options. In other words, there’s a clear trend for learning: explore all your options when you’re unaware of them, and gradually shift over to exploiting once you’ve established opinions on some of them. In the same manner, we want our model to capture this natural model of learning, and epsilon plays that role.</p><p id="4031" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Epsilon denotes the fraction of time we will dedicate to exploring. That is, a fraction <em class="iv">self.epsilon </em>of the trials, we will simply take a random action rather than the one we would predict to be the best in that scenario. As stated, we want to do this more often than not in the beginning, before we form stabilizing valuations on the matter, and so initialize epsilon to close to 1.0 at the beginning and decay it by some fraction &lt;1 at every successive time step.</p><p id="f772" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">DQN Models</strong></p><p id="621b" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">There was one key thing that was excluded in the initialization of the DQN above: the actual model used for predictions! As in our original Keras RL tutorial, we are directly given the input and output as numeric vectors. So, there’s no need to employ more complex layers in our network other than fully connected layers. Specifically, we define our model just as:</p><pre class="hg hh hi hj hk iy iz cm"><span id="c9a6" class="ja jb ef at jc b fi jd je r jf" data-selectable-paragraph="">def create_model(self):<br>        model   = Sequential()<br>        state_shape  = self.env.observation_space.shape<br>        model.add(Dense(24, input_dim=state_shape[0], <br>            activation="relu"))<br>        model.add(Dense(48, activation="relu"))<br>        model.add(Dense(24, activation="relu"))<br>        model.add(Dense(self.env.action_space.n))<br>        model.compile(loss="mean_squared_error",<br>            optimizer=Adam(lr=self.learning_rate))<br>        return model</span></pre><p id="9c18" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">And use this to define the model and target model (explained below):</p><pre class="hg hh hi hj hk iy iz cm"><span id="650d" class="ja jb ef at jc b fi jd je r jf" data-selectable-paragraph="">def __init__(self, env):<br>        self.env     = env<br>        self.memory  = deque(maxlen=2000)<br>        <br>        self.gamma = 0.95<br>        self.epsilon = 1.0<br>        self.epsilon_min = 0.01<br>        self.epsilon_decay = 0.995<br>        self.learning_rate = 0.01<br>        self.tau = .05</span><span id="99ea" class="ja jb ef at jc b fi jg jh ji jj jk je r jf" data-selectable-paragraph="">        self.model = self.create_model()<br>        # "hack" implemented by DeepMind to improve convergence<br>        self.target_model = self.create_model()</span></pre><p id="24b0" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">The fact that there are two <em class="iv">separate </em>models, one for doing predictions and one for tracking “target values” is definitely counter-intuitive. To be explicit, the role of the model (<em class="iv">self.model</em>) is to do the actual predictions on what action to take, and the target model (<em class="iv">self.target_model</em>) tracks what action we <em class="iv">want </em>our model to take.</p><p id="d789" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Why not just have a single model that does both? After all, if something is predicting the action to take, shouldn’t it be implicitly determine what model we <em class="iv">want </em>our model to take? This is actually one of those “weird tricks” in deep learning that DeepMind developed to get convergence in the DQN algorithm. If you use a single model, it can (and often does) converge in simple environments (such as the CartPole). But, the reason it doesn’t converge in these more complex environments is because of how we’re training the model: as mentioned previously, we’re training it “on the fly.”</p><p id="89aa" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">As a result, we are doing training at each time step and, if we used a single network, would also be essentially changing the “goal” at each time step. Think of how confusing that would be! That would be like if a teacher told you to go finish pg. 6 in your textbook and, by the time you finished half of it, she changed it to pg. 9, and by the time you finished half of that, she told you to do pg. 21! This, therefore, causes a lack of convergence by a lack of clear direction in which to employ the optimizer, i.e. the gradients are changing too rapidly for stable convergence. So, to compensate, we have a network that changes more slowly that tracks our eventual goal and one that is trying to achieve those.</p><p id="56c5" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">DQN Training</strong></p><p id="3d7f" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">The training involves three main steps: remembering, learning, and reorienting goals. The first is basically just adding to the memory as we go through more trials:</p><pre class="hg hh hi hj hk iy iz cm"><span id="0a9c" class="ja jb ef at jc b fi jd je r jf" data-selectable-paragraph="">def remember(self, state, action, reward, new_state, done):<br>        self.memory.append([state, action, reward, new_state, done])</span></pre><p id="f42c" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">There’s not much of note here other than that we have to store the <em class="iv">done</em> phase for how we later update the reward function. Moving on to the main body of our DQN, we have the train function. This is where we make use of our stored memory and actively learn from what we’ve seen in the past. We start by taking a sample from our entire memory storage. From there, we handle each sample different. As we saw in the equation before, we want to update the <em class="iv">Q </em>function as <strong class="gs he">the sum of </strong>the current reward <strong class="gs he">and </strong>expected future rewards (depreciated by gamma). In the case we are at the end of the trials, there are no such future rewards, so the entire value of this state is just the current reward we received. In a non-terminal state, however, we want to see what the maximum reward we would receive would be if we were able to take any possible action, from which we get:</p><pre class="hg hh hi hj hk iy iz cm"><span id="da9d" class="ja jb ef at jc b fi jd je r jf" data-selectable-paragraph="">def replay(self):<br>        batch_size = 32<br>        if len(self.memory) &lt; batch_size: <br>            return</span><span id="4f3c" class="ja jb ef at jc b fi jg jh ji jj jk je r jf" data-selectable-paragraph="">        samples = random.sample(self.memory, batch_size)<br>        for sample in samples:<br>            state, action, reward, new_state, done = sample<br>            target = self.target_model.predict(state)<br>            if done:<br>                target[0][action] = reward<br>            else:<br>                Q_future = max(<br>                    self.target_model.predict(new_state)[0])<br>                target[0][action] = reward + Q_future * self.gamma<br>            self.model.fit(state, target, epochs=1, verbose=0)</span></pre><p id="43b3" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">And finally, we have to reorient our goals, where we simply copy over the weights from the main model into the target one. Unlike the main train method, however, this target update is called less frequently:</p><pre class="hg hh hi hj hk iy iz cm"><span id="dfaf" class="ja jb ef at jc b fi jd je r jf" data-selectable-paragraph="">def target_train(self):<br>        weights = self.model.get_weights()<br>        target_weights = self.target_model.get_weights()<br>        for i in range(len(target_weights)):<br>            target_weights[i] = weights[i]<br>        self.target_model.set_weights(target_weights)</span></pre><p id="5b30" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">DQN Action</strong></p><p id="6310" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">The final step is simply getting the DQN to actually perform the desired action, which alternates based on the given epsilon parameter between taking a random action and one predicated on past training, as follows:</p><pre class="hg hh hi hj hk iy iz cm"><span id="3eab" class="ja jb ef at jc b fi jd je r jf" data-selectable-paragraph="">def act(self, state):<br>        self.epsilon *= self.epsilon_decay<br>        self.epsilon = max(self.epsilon_min, self.epsilon)<br>        if np.random.random() &lt; self.epsilon:<br>            return self.env.action_space.sample()<br>        return np.argmax(self.model.predict(state)[0])</span></pre><p id="ca32" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">Training Agent</strong></p><p id="1973" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Training the agent now follows naturally from the complex agent we developed. We have to instantiate it, feed it the experiences as we encounter them, train the agent, and update the target network:</p><pre class="hg hh hi hj hk iy iz cm"><span id="8934" class="ja jb ef at jc b fi jd je r jf" data-selectable-paragraph="">def main():<br>    env     = gym.make("MountainCar-v0")<br>    gamma   = 0.9<br>    epsilon = .95</span><span id="0660" class="ja jb ef at jc b fi jg jh ji jj jk je r jf" data-selectable-paragraph="">    trials  = 100<br>    trial_len = 500</span><span id="2de3" class="ja jb ef at jc b fi jg jh ji jj jk je r jf" data-selectable-paragraph="">    updateTargetNetwork = 1000<br>    dqn_agent = DQN(env=env)<br>    steps = []<br>    for trial in range(trials):<br>        cur_state = env.reset().reshape(1,2)<br>        for step in range(trial_len):<br>            action = dqn_agent.act(cur_state)<br>            env.render()<br>            new_state, reward, done, _ = env.step(action)</span><span id="5989" class="ja jb ef at jc b fi jg jh ji jj jk je r jf" data-selectable-paragraph="">            reward = reward if not done else -20<br>            print(reward)<br>            new_state = new_state.reshape(1,2)<br>            dqn_agent.remember(cur_state, action, <br>                reward, new_state, done)<br>            <br>            dqn_agent.replay()<br>            dqn_agent.target_train()</span><span id="0eee" class="ja jb ef at jc b fi jg jh ji jj jk je r jf" data-selectable-paragraph="">            cur_state = new_state<br>            if done:<br>                break<br>        if step &gt;= 199:<br>            print("Failed to complete trial")<br>        else:<br>            print("Completed in {} trials".format(trial))<br>            break</span></pre><p id="9efa" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph=""><strong class="gs he">Complete Code</strong></p><p id="94b5" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">With that, here is the complete code used for training against the “MountainCar-v0” environment using DQN!</p><figure class="hg hh hi hj hk hl"><div class="hr r hs"><div class="qp r"><iframe src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/d989aded95a0d7dd02464fc56ddbb54e.html" allowfullscreen="" frameborder="0" height="2450" width="680" title="" class="ds t u ho ak" scrolling="auto"></iframe></div></div></figure><p id="fd10" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx" data-selectable-paragraph="">Keep an eye out for the next Keras+OpenAI tutorial!</p><h1 id="0ff4" class="jm jb ef at as jn eh jo ej jp jq jr js jt ju jv jw" data-selectable-paragraph="">Comment and click that ❤️ below to show support!</h1></div></div></section></div></article><div class="nd dw jx s ak ke kc kf" data-test-id="post-sidebar"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="kg n dr"><div class="qg"><div class="kh ki r"><a href="https://towardsdatascience.com/?source=post_sidebar--------------------------post_sidebar-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><h2 class="as jn kj au ef">Towards Data Science</h2></a><div class="kk kl r"><h4 class="as cx fi au cd km fk fl kn fn ax">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="bx" aria-hidden="true"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=post_sidebar--------------------------follow_sidebar-" class="ko bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div><div class="kp kq kr n"><div class="n o"><div class="ks r hs"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=post_sidebar-----1eed3a5338c---------------------clap_sidebar-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><div class="bg kt ku kv kw kx ky kz ba la bk"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="lb r"><div class="lc"><h4 class="as cx fi au ax"><button class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq">1.2K </button></h4></div></div></div></div><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=post_sidebar--------------------------bookmark_sidebar-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div><div class="nd qg jx s jy jz ka kb kc kd"></div><div><div class="ld hl n dr p"><div class="n p"><div class="ac ae af ag ah ec aj ak"><div class="n le"></div><div class="n o le"></div><div class="lf r"><ul class="bg bh"><li class="bx cj lg lh"><a href="https://towardsdatascience.com/tag/machine-learning" class="li lj by ax r iz lk a b cy">Machine Learning</a></li><li class="bx cj lg lh"><a href="https://towardsdatascience.com/tag/keras" class="li lj by ax r iz lk a b cy">Keras</a></li><li class="bx cj lg lh"><a href="https://towardsdatascience.com/tag/deep-leraning" class="li lj by ax r iz lk a b cy">Deep Leraning</a></li><li class="bx cj lg lh"><a href="https://towardsdatascience.com/tag/reinforcement-learning" class="li lj by ax r iz lk a b cy">Reinforcement Learning</a></li><li class="bx cj lg lh"><a href="https://towardsdatascience.com/tag/towards-data-science" class="li lj by ax r iz lk a b cy">Towards Data Science</a></li></ul></div><div class="ll n ey ab"><div class="n o"><div class="lm r hs"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=post_actions_footer-----1eed3a5338c---------------------clap_footer-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><div class="c ln fc n o lo hs lp lq lr ls lt lu lv lw lx ly lz ma mb bs"><div class="bg kt ku kv kw kx mc kz o hy fc n p md u ho ds t ak ba la bk me"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></div></div></a></div><div class="lb r"><div class="lc"><h4 class="as cx fi au ef"><button class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq">1.2K claps</button></h4></div></div></div><div class="n o"><div class="gm r ap"><a href="https://medium.com/p/1eed3a5338c/share/twitter?source=post_actions_footer---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gm r ap"><a href="https://medium.com/p/1eed3a5338c/share/facebook?source=post_actions_footer---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gm r ap"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=post_actions_footer--------------------------bookmark_sidebar-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div><div class="bx" aria-hidden="true"><div class="bx" aria-hidden="true"><div class="r ap"><button class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div><div class="mf mg mh lf r ab"><div class="mi mj r hs"><span class="r mk an ml"><div class="r ds mm mn"><a href="https://towardsdatascience.com/@yashpatel_86510?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Yash Patel" class="r fc cq mo" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1____MnwCLk7NXM6xZIipDBA(1).png" width="80" height="80"></a></div><span class="r"><div class="mp r mq"><p class="as cx cy au ax da mr">Written by</p></div><div class="mp ms n mq"><div class="ak n o ey"><h2 class="as jn mt mu ef"><a href="https://towardsdatascience.com/@yashpatel_86510?source=follow_footer--------------------------follow_footer-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener">Yash Patel</a></h2><div class="r g"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=follow_footer-68694da71a5f-------------------------follow_footer-" class="ko bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></div></span></span><div class="mp mv r mq cb"><div class="mw r"><h4 class="as cx kj mx ax">Developer interested in Computer Vision, Graphics, and VR working at Oculus VR (Facebook). Graduate from Princeton University (http://www.ypatel.io)</h4></div><div class="my mz cb"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=follow_footer-68694da71a5f-------------------------follow_footer-" class="ko bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></div><div class="mf r"></div><div class="mi mj r hs"><span class="r mk an ml"><div class="r ds mm mn"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Towards Data Science" class="bt mo cq" src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/1_hVxgUA6kP-PgL5TJjuyePg.png" width="80" height="80"></a></div><span class="r"><div class="mp ms n mq"><div class="ak n o ey"><h2 class="as jn mt mu ef"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener">Towards Data Science</a></h2><div class="r g"><div class="bx" aria-hidden="true"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=follow_footer--------------------------follow_footer-" class="ko bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></div></div></span></span><div class="mp na r mq cb"><div class="mw r"><h4 class="as cx kj mx ax">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="my mz cb"><div class="bx" aria-hidden="true"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c&amp;source=follow_footer--------------------------follow_footer-" class="ko bq az ba br bj bk bs bi bt as b at au av aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></div></div></div><div class="nb mg r ab"><a href="https://medium.com/p/1eed3a5338c/responses/show?source=follow_footer--------------------------follow_footer-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><span class="nc nd kw"><div class="iy ne bt r ib cb"><span class="az">See responses (9)</span></div></span></a></div></div></div><div class="nf r ng ab"><div class="n p"><div class="ac ae af ag ah ai aj ak"></div></div></div></div></div><div class="nh r ni nj"><section class="do dp ak bw r nk nl nm nn no np nq nr ns nt nu nv nw nx ny"><div class="nz oa mi n ey g"><div class="ob n ey"><div class="oc r od"><div class="oe r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi of og bl bm oh oi" rel="noopener"><h4 class="oj ok ol as jn at mx om on r">Discover <!-- -->Medium</h4></a></div><span class="as b at au av aw r oo op">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi bl bm oh oi oq" rel="noopener">Watch</a></span></div><div class="oc r od"><div class="or r"><a href="https://medium.com/topics?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi of og bl bm oh oi" rel="noopener"><h4 class="oj ok ol as jn at mx om on r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="as b at au av aw r oo op">Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi bl bm oh oi oq" rel="noopener">Explore</a></span></div><div class="oc r od"><div class="oe r"><a href="https://medium.com/membership?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi of og bl bm oh oi" rel="noopener"><h4 class="oj ok ol as jn at mx om on r">Become a member</h4></a></div><span class="as b at au av aw r oo op">Get unlimited access to the best stories on <!-- -->Medium<!-- --> — and support writers while you’re at it. Just $5/month.<!-- --> <a href="https://medium.com/membership?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi bl bm oh oi oq" rel="noopener">Upgrade</a></span></div></div></div><div class="n o ey"><a href="https://medium.com/?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi of og bl bm oh oi" rel="noopener"><svg height="22" width="112" viewBox="0 0 111.5 22" class="ok"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="as b at au av aw r oo op"><div class="os ot n ey ou an"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi fo bl bm oh oi" rel="noopener">About</a><a href="https://help.medium.com/?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi fo bl bm oh oi" rel="noopener">Help</a><a href="https://medium.com/policy/9db0094a1e0f?source=post_page-----1eed3a5338c----------------------" class="dc dd bb bc bd be bf bg bh bi fo bl bm oh oi" rel="noopener">Legal</a></div></span></div></section></div></div></div><script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/16180790160.js"></script><iframe src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/a16180790160.html" hidden="" aria-hidden="true" tabindex="-1" title="Optimizely Internal Frame" height="0" width="0" style="display: none;"></iframe><script>window.__BUILD_ID__ = "development"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200128-214639-ea64530f33","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200128-214639-ea64530f33"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440}},"performanceTags":[]},"debug":{"requestId":"9a05f992-3a90-4830-b60f-63727c155f7b","originalSpanCarrier":{"ot-tracer-spanid":"39f133ce1cfb17c0","ot-tracer-traceid":"0eb49ba06eed9b94","ot-tracer-sampled":"true"}},"session":{"user":{"id":"lo_6rS0xWOMhJgo"},"xsrf":""},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"showBranchBanner":null,"hasRenderedBranchBanner":null,"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"branchData":{"loaded":false},"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","susiModal":{"step":null,"operation":"register","reportEventInfo":{"eventName":"","data":{}}}},"client":{"isBot":false,"isEu":true,"isLinkedin":false,"isNativeMedium":false,"isCustomDomain":true},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.9":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.10":{"name":"disable_go_social","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_auto_tier","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_geolocation_stripe_standard","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.36":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_lite_membership_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagString","value":"new-content"},"ROOT_QUERY.variantFlags.42":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_lo_homepage","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_more_branch_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_new_pub_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_nonmember_reading_list_web","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_recently_viewed_tab_on_web","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_send_event_for_active_users_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_sign_up_with_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_suggest_account","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_suggest_account_li","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.84":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.86":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.89":{"name":"pardon_the_interruption_4","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.90":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.92":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.93":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.94":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.94.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.94.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.95":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.95.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.95.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.94","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.95","typename":"VariantFlag"}],"viewer":null,"meterPost({\"postId\":\"1eed3a5338c\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"1eed3a5338c\"})":{"type":"id","generated":false,"id":"Post:1eed3a5338c","typename":"Post"}},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":3,"unlocksRemaining":3},"Post:1eed3a5338c":{"__typename":"Post","visibility":"PUBLIC","latestPublishedVersion":"1db99ddd69b6","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"id":"1eed3a5338c","creator":{"type":"id","generated":false,"id":"User:68694da71a5f","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","sequence":null,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Freinforcement-learning-w-keras-openai-dqns-1eed3a5338c","canonicalUrl":"","content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":true,"id":"$Post:1eed3a5338c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})","typename":"PostContent"},"firstPublishedAt":1501398157835,"isPublished":true,"layerCake":0,"primaryTopic":null,"title":"Reinforcement Learning w\u002F Keras + OpenAI: DQNs","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":"APPROVED","readingTime":10.289622641509435,"readingList":"READING_LIST_NONE","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:keras","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-leraning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:reinforcement-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:towards-data-science","typename":"Tag"}],"viewerClapCount":0,"clapCount":1211,"voterCount":221,"recommenders":[],"responsesCount":9,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"isDistributionAlertDismissed":false,"audioVersionUrl":"","seoTitle":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1501402933411,"previewContent":{"type":"id","generated":true,"id":"$Post:1eed3a5338c.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*nbCSvWmyS_BUDz_WAJyKUw.gif","typename":"ImageMetadata"},"updatedAt":1529571213888,"topics":[],"seoDescription":"","isSuspended":false},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","domain":"towardsdatascience.com","slug":"towards-data-science","__typename":"Collection","auroraAlphaEnabled":false,"googleAnalyticsId":"UA-19707169-24","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","name":"Towards Data Science","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","typename":"ImageMetadata"},"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"creator":{"type":"id","generated":false,"id":"User:895063a310f4","typename":"User"},"viewerIsEditor":false,"navItems":[{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.4","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.5","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.6","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.7","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"A Medium publication sharing concepts, ideas, and codes.","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"isUserSubscribedToCollectionEmails":false,"ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"favicon":{"type":"id","generated":false,"id":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png","typename":"ImageMetadata"}},"User:68694da71a5f":{"id":"68694da71a5f","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Yash Patel","isFollowing":false,"username":"yashpatel_86510","bio":"Developer interested in Computer Vision, Graphics, and VR working at Oculus VR (Facebook). Graduate from Princeton University (http:\u002F\u002Fwww.ypatel.io)","imageId":"1*___MnwCLk7NXM6xZIipDBA.png","mediumMemberAt":0,"isBlocking":false,"isPartnerProgramEnrolled":false,"twitterScreenName":""},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","originalWidth":337,"originalHeight":122,"__typename":"ImageMetadata"},"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png":{"id":"1*hVxgUA6kP-PgL5TJjuyePg.png","__typename":"ImageMetadata"},"User:895063a310f4":{"id":"895063a310f4","__typename":"User"},"Collection:7f60cf5620c9.navItems.0":{"title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.1":{"title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.2":{"title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.3":{"title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.4":{"title":"AI","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fartificial-intelligence\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.5":{"title":"Picks","url":"https:\u002F\u002Ftowardsdatascience.com\u002Four-picks\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.6":{"title":"More","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmore\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.7":{"title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF355876","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF355876","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF4D6C88","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF637F99","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF7791A8","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF8CA2B7","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FF9FB3C6","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FFB2C3D4","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFC5D2E1","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFD7E2EE","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFE9F1FA","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFFBFFFF","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"},"highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum","typename":"ColorSpectrum"}},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF668AAA","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF61809D","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF5A7690","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF546C83","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF4D6275","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF455768","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF3D4C5A","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF34414C","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF2B353E","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF21282F","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF161B1F","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFEDF4FC","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFE9F2FD","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFE6F1FD","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFE2EFFD","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFDFEEFD","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFDBECFE","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFD7EBFE","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFD4E9FE","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFD0E7FF","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFCCE6FF","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFC8E4FF","point":1,"__typename":"ColorPoint"},"$Post:1eed3a5338c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:1eed3a5338c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel","typename":"RichText"}},"$Post:1eed3a5338c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0":{"name":"9682","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:1eed3a5338c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:1eed3a5338c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:1db99ddd69b6_60","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:1db99ddd69b6_0":{"id":"1db99ddd69b6_0","name":"12b9","type":"H3","href":null,"layout":null,"metadata":null,"text":"Reinforcement Learning w\u002F Keras + OpenAI: DQNs","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_1":{"id":"1db99ddd69b6_1","name":"dc33","type":"P","href":null,"layout":null,"metadata":null,"text":"Quick Recap","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_1.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_1.markups.0":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_2":{"id":"1db99ddd69b6_2","name":"ce16","type":"P","href":null,"layout":null,"metadata":null,"text":"Last time in our Keras\u002FOpenAI tutorial, we discussed a very basic example of applying deep learning to reinforcement learning contexts. This was an incredible showing in retrospect! If you looked at the training data, the random chance models would usually only be able to perform for 60 steps in median. And yet, by training on this seemingly very mediocre data, we were able to “beat” the environment (i.e. get \u003E200 step performance). How is this possible?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_3":{"id":"1db99ddd69b6_3","name":"fd26","type":"P","href":null,"layout":null,"metadata":null,"text":"We can get directly an intuitive feel for this. Let’s imagine the perfectly random series we used as our training data. It is extremely unlikely that any two series will have high overlap with one another, since these are generated completely randomly. However, there are key features that are common between successful trials, such as pushing the cart right when the pole is leaning right and vice versa. And so, by training our NN on all these trials data, we extract the shared patterns that contributed to them being successful and are able to smooth over the details that resulted in their independent failures.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_3.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_3.markups.0":{"type":"STRONG","start":268,"end":271,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_4":{"id":"1db99ddd69b6_4","name":"4a08","type":"P","href":null,"layout":null,"metadata":null,"text":"That being said, the environment we consider this week is significantly more difficult than that from last week: the MountainCar.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_5":{"id":"1db99ddd69b6_5","name":"d773","type":"P","href":null,"layout":null,"metadata":null,"text":"More Complex Environments","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_5.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_5.markups.0":{"type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_6":{"id":"1db99ddd69b6_6","name":"f202","type":"P","href":null,"layout":null,"metadata":null,"text":"Even though it seems we should be able to apply the same technique as that we applied last week, there is one key features here that makes doing so impossible: we can’t generate training data. Unlike the very simple Cartpole example, taking random movements often simply leads to the trial ending in us at the bottom of the hill. That is, we have several trials that are all identically -200 in the end. This is practically useless to use as training data. Imagine you were in a class where no matter what answers you put on your exam, you got a 0%! How are you going to learn from any of those experiences?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_7":{"id":"1db99ddd69b6_7","name":"319a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*nbCSvWmyS_BUDz_WAJyKUw.gif","typename":"ImageMetadata"},"text":"Random inputs for the “MountainCar-v0” environment does not produce any output that is worthwhile or useful to train on","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*nbCSvWmyS_BUDz_WAJyKUw.gif":{"id":"1*nbCSvWmyS_BUDz_WAJyKUw.gif","originalHeight":396,"originalWidth":598,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:1db99ddd69b6_8":{"id":"1db99ddd69b6_8","name":"4849","type":"P","href":null,"layout":null,"metadata":null,"text":"In line with that, we have to figure out a way to incrementally improve upon previous trials. For this, we use one of the most basic stepping stones for reinforcement learning: Q-learning!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_9":{"id":"1db99ddd69b6_9","name":"46a4","type":"P","href":null,"layout":null,"metadata":null,"text":"DQN Theory Background","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_9.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_9.markups.0":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_10":{"id":"1db99ddd69b6_10","name":"97ca","type":"P","href":null,"layout":null,"metadata":null,"text":"Q-learning (which doesn’t stand for anything, by the way) is centered around creating a “virtual table” that accounts for how much reward is assigned to each possible action given the current state of the environment. Let’s break that down one step at a time:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_11":{"id":"1db99ddd69b6_11","name":"e1d4","type":"IMG","href":null,"layout":"OUTSET_LEFT","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*WEfdub-U4i5HRLunOlXj8g.png","typename":"ImageMetadata"},"text":"You can imagine the DQN network as internally maintaining a spreadsheet of the values of each of the possible actions that can be taken given the current environment state","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*WEfdub-U4i5HRLunOlXj8g.png":{"id":"1*WEfdub-U4i5HRLunOlXj8g.png","originalHeight":224,"originalWidth":686,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:1db99ddd69b6_12":{"id":"1db99ddd69b6_12","name":"84af","type":"P","href":null,"layout":null,"metadata":null,"text":"What do we mean by “virtual table?” Imagine that for each possible configuration of the input space, you have a table that assigns a score for each of the possible actions you can take. If this were magically possible, then it would be extremely easy for you to “beat” the environment: simply choose the action that has the highest score! Two points to note about this score. First, this score is conventionally referred to as the “Q-score,” which is where the name of the overall algorithm comes from. Second, as with any other score, these Q score have no meaning outside the context of their simulation. That is, they have no absolute significance, but that’s perfectly fine, since we solely need it to do comparisons.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_12.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_12.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_12.markups.0":{"type":"STRONG","start":626,"end":650,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_12.markups.1":{"type":"EM","start":555,"end":566,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_13":{"id":"1db99ddd69b6_13","name":"cfbe","type":"P","href":null,"layout":null,"metadata":null,"text":"Why then do we need virtual table for each input configuration? Why can’t we just have one table to rule them all? The reason is that it doesn’t make sense to do so: that would be the same as saying the best action to take while at the bottom of the valley is exactly that which you should take when you are perched on the highest point of the left incline.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_13.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_13.markups.0":{"type":"EM","start":38,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_14":{"id":"1db99ddd69b6_14","name":"984c","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, the main problem with what I described (maintaining a virtual table for each input configuration) is that this is impossible: we have a continuous (infinite) input space! We could get around this by discretizing the input space, but that seems like a pretty hacky solution to this problem that we’ll be encountering over and over in future situations. So, how do we get around this? By applying neural nets to the situation: that’s where the D in DQN comes from!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_14.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_14.markups.0":{"type":"EM","start":77,"end":82,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_15":{"id":"1db99ddd69b6_15","name":"de5c","type":"P","href":null,"layout":null,"metadata":null,"text":"DQN Agent","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_15.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_15.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_16":{"id":"1db99ddd69b6_16","name":"ef63","type":"P","href":null,"layout":null,"metadata":null,"text":"So, we’ve now reduced the problem to finding a way to assign the different actions Q-scores given the current state. This is the answer to a very natural first question to answer when employing any NN: what are the inputs and outputs of our model? The extent of the math you need to understand for this model is the following equation (don’t worry, we’ll break it down):","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_17":{"id":"1db99ddd69b6_17","name":"268b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*IyrdggWA1wsce4nBA6u2VA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*IyrdggWA1wsce4nBA6u2VA.png":{"id":"1*IyrdggWA1wsce4nBA6u2VA.png","originalHeight":226,"originalWidth":740,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:1db99ddd69b6_18":{"id":"1db99ddd69b6_18","name":"0d55","type":"P","href":null,"layout":null,"metadata":null,"text":"Q, as mentioned, represents the value estimated by our model given the current state (s) and action taken (a). The goal, however, is to determine the overall value of a state. What do I mean by that? The overall value is both the immediate reward you will get and the expected rewards you will get in the future from being in that position. That is, we want to account for the fact that the value of a position often reflects not only its immediate gains but also the future gains it enables (damn, deep). In any case, we discount future rewards because, if I compare two situations in which I expect to get $100 one of the two being in the future, I would always take the present deal, since the position of the future one may change between when I made the deal and when I receive the money. The gamma factor reflects this depreciated value for the expected future returns on the state.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_18.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_18.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_18.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_18.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_18.markups.0":{"type":"STRONG","start":221,"end":226,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_18.markups.1":{"type":"STRONG","start":260,"end":264,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_18.markups.2":{"type":"EM","start":150,"end":158,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_18.markups.3":{"type":"EM","start":204,"end":212,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_19":{"id":"1db99ddd69b6_19","name":"6ab5","type":"P","href":null,"layout":null,"metadata":null,"text":"And that’s it: that’s all the math we’ll need for this! Time to actually move on to some code!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_20":{"id":"1db99ddd69b6_20","name":"a607","type":"P","href":null,"layout":null,"metadata":null,"text":"DQN Agent Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_20.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_20.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_21":{"id":"1db99ddd69b6_21","name":"01cd","type":"P","href":null,"layout":null,"metadata":null,"text":"The Deep Q Network revolves around continuous learning, meaning that we don’t simply accrue a bunch of trial\u002Ftraining data and feed it into the model. Instead, we create training data through the trials we run and feed this information into it directly after running the trial. If this all seems somewhat vague right now, don’t worry: time to see some code about this. The code largely revolves around defining a DQN class, where all the logic of the algorithm will actually be implemented, and where we expose a simple set of functions for the actual training.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_22":{"id":"1db99ddd69b6_22","name":"b899","type":"P","href":null,"layout":null,"metadata":null,"text":"DQN Hyperparameters","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_22.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_22.markups.0":{"type":"STRONG","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_23":{"id":"1db99ddd69b6_23","name":"6893","type":"P","href":null,"layout":null,"metadata":null,"text":"First off, we’re going to discuss some parameters of relevance for DQNs. Most of them are standard from most neural net implementations:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_24":{"id":"1db99ddd69b6_24","name":"fbb4","type":"PRE","href":null,"layout":null,"metadata":null,"text":"class DQN:\n    def __init__(self, env):\n        self.env     = env\n        self.memory  = deque(maxlen=2000)\n        \n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.01","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_25":{"id":"1db99ddd69b6_25","name":"ab97","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s step through these one at a time. The first is simply the environment, which we supply for convenience when we need to reference the shapes in creating our model. The “memory” is a key component of DQNs: as mentioned previously, the trials are used to continuously train the model. However, rather than training on the trials as they come in, we add them to memory and train on a random sample of that memory. Why do this instead of just training on the last x trials as our “sample?” The reason is somewhat subtle. Imagine instead we were to just train on the most recent trials as our sample: in this case, our results would only learn on its most recent actions, which may not be directly relevant for future predictions. In this environment in particular, if we were moving down the right side of the slope, training on the most recent trials would entail training on the data where you were moving up the hill towards the right. But, this would not be at all relevant to determining what actions to take in the scenario you would soon be facing of scaling up the left hill. So, by taking a random sample, we don’t bias our training set, and instead ideally learn about scaling all environments we would encounter equally well.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_25.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_25.markups.0":{"type":"EM","start":465,"end":467,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_26":{"id":"1db99ddd69b6_26","name":"b6a4","type":"P","href":null,"layout":null,"metadata":null,"text":"So, we now discuss hyperparameters of the model: gamma, epsilon\u002Fepsilon decay, and the learning rate. The first is the future rewards depreciation factor (\u003C1) discussed in the earlier equation, and the last is the standard learning rate parameter, so I won’t discuss that here. The second, however, is an interesting facet of RL that deserves a moment to discuss. In any sort of learning experience, we always have the choice between exploration vs. exploitation. This isn’t limited to computer science or academics: we do this on a day to day basis!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_27":{"id":"1db99ddd69b6_27","name":"2e01","type":"P","href":null,"layout":null,"metadata":null,"text":"Consider the restaurants in your local neighborhood. When was the last time you went to a new one? Probably a long time ago. That corresponds to your shift from exploration to exploitation: rather than trying to find new and better opportunities, you settle with the best one you’ve found in your past experiences and maximize your utility from there. Contrast that to when you moved into your house: at that time, you had no idea what restaurants were good or not and so were enticed to explore your options. In other words, there’s a clear trend for learning: explore all your options when you’re unaware of them, and gradually shift over to exploiting once you’ve established opinions on some of them. In the same manner, we want our model to capture this natural model of learning, and epsilon plays that role.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_27.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_27.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_27.markups.0":{"type":"EM","start":161,"end":173,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_27.markups.1":{"type":"EM","start":176,"end":188,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_28":{"id":"1db99ddd69b6_28","name":"4031","type":"P","href":null,"layout":null,"metadata":null,"text":"Epsilon denotes the fraction of time we will dedicate to exploring. That is, a fraction self.epsilon of the trials, we will simply take a random action rather than the one we would predict to be the best in that scenario. As stated, we want to do this more often than not in the beginning, before we form stabilizing valuations on the matter, and so initialize epsilon to close to 1.0 at the beginning and decay it by some fraction \u003C1 at every successive time step.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_28.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_28.markups.0":{"type":"EM","start":88,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_29":{"id":"1db99ddd69b6_29","name":"f772","type":"P","href":null,"layout":null,"metadata":null,"text":"DQN Models","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_29.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_29.markups.0":{"type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_30":{"id":"1db99ddd69b6_30","name":"621b","type":"P","href":null,"layout":null,"metadata":null,"text":"There was one key thing that was excluded in the initialization of the DQN above: the actual model used for predictions! As in our original Keras RL tutorial, we are directly given the input and output as numeric vectors. So, there’s no need to employ more complex layers in our network other than fully connected layers. Specifically, we define our model just as:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_31":{"id":"1db99ddd69b6_31","name":"c9a6","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def create_model(self):\n        model   = Sequential()\n        state_shape  = self.env.observation_space.shape\n        model.add(Dense(24, input_dim=state_shape[0], \n            activation=\"relu\"))\n        model.add(Dense(48, activation=\"relu\"))\n        model.add(Dense(24, activation=\"relu\"))\n        model.add(Dense(self.env.action_space.n))\n        model.compile(loss=\"mean_squared_error\",\n            optimizer=Adam(lr=self.learning_rate))\n        return model","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_32":{"id":"1db99ddd69b6_32","name":"9c18","type":"P","href":null,"layout":null,"metadata":null,"text":"And use this to define the model and target model (explained below):","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_33":{"id":"1db99ddd69b6_33","name":"650d","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def __init__(self, env):\n        self.env     = env\n        self.memory  = deque(maxlen=2000)\n        \n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.01\n        self.tau = .05","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_34":{"id":"1db99ddd69b6_34","name":"99ea","type":"PRE","href":null,"layout":null,"metadata":null,"text":"        self.model = self.create_model()\n        # \"hack\" implemented by DeepMind to improve convergence\n        self.target_model = self.create_model()","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_35":{"id":"1db99ddd69b6_35","name":"24b0","type":"P","href":null,"layout":null,"metadata":null,"text":"The fact that there are two separate models, one for doing predictions and one for tracking “target values” is definitely counter-intuitive. To be explicit, the role of the model (self.model) is to do the actual predictions on what action to take, and the target model (self.target_model) tracks what action we want our model to take.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_35.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_35.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_35.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_35.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_35.markups.0":{"type":"EM","start":28,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_35.markups.1":{"type":"EM","start":180,"end":190,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_35.markups.2":{"type":"EM","start":270,"end":287,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_35.markups.3":{"type":"EM","start":311,"end":316,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_36":{"id":"1db99ddd69b6_36","name":"d789","type":"P","href":null,"layout":null,"metadata":null,"text":"Why not just have a single model that does both? After all, if something is predicting the action to take, shouldn’t it be implicitly determine what model we want our model to take? This is actually one of those “weird tricks” in deep learning that DeepMind developed to get convergence in the DQN algorithm. If you use a single model, it can (and often does) converge in simple environments (such as the CartPole). But, the reason it doesn’t converge in these more complex environments is because of how we’re training the model: as mentioned previously, we’re training it “on the fly.”","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_36.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_36.markups.0":{"type":"EM","start":158,"end":163,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_37":{"id":"1db99ddd69b6_37","name":"89aa","type":"P","href":null,"layout":null,"metadata":null,"text":"As a result, we are doing training at each time step and, if we used a single network, would also be essentially changing the “goal” at each time step. Think of how confusing that would be! That would be like if a teacher told you to go finish pg. 6 in your textbook and, by the time you finished half of it, she changed it to pg. 9, and by the time you finished half of that, she told you to do pg. 21! This, therefore, causes a lack of convergence by a lack of clear direction in which to employ the optimizer, i.e. the gradients are changing too rapidly for stable convergence. So, to compensate, we have a network that changes more slowly that tracks our eventual goal and one that is trying to achieve those.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_38":{"id":"1db99ddd69b6_38","name":"56c5","type":"P","href":null,"layout":null,"metadata":null,"text":"DQN Training","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_38.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_38.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_39":{"id":"1db99ddd69b6_39","name":"3d7f","type":"P","href":null,"layout":null,"metadata":null,"text":"The training involves three main steps: remembering, learning, and reorienting goals. The first is basically just adding to the memory as we go through more trials:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_40":{"id":"1db99ddd69b6_40","name":"0a9c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def remember(self, state, action, reward, new_state, done):\n        self.memory.append([state, action, reward, new_state, done])","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_41":{"id":"1db99ddd69b6_41","name":"f42c","type":"P","href":null,"layout":null,"metadata":null,"text":"There’s not much of note here other than that we have to store the done phase for how we later update the reward function. Moving on to the main body of our DQN, we have the train function. This is where we make use of our stored memory and actively learn from what we’ve seen in the past. We start by taking a sample from our entire memory storage. From there, we handle each sample different. As we saw in the equation before, we want to update the Q function as the sum of the current reward and expected future rewards (depreciated by gamma). In the case we are at the end of the trials, there are no such future rewards, so the entire value of this state is just the current reward we received. In a non-terminal state, however, we want to see what the maximum reward we would receive would be if we were able to take any possible action, from which we get:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_41.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_41.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_41.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_41.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_41.markups.0":{"type":"STRONG","start":465,"end":476,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_41.markups.1":{"type":"STRONG","start":495,"end":499,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_41.markups.2":{"type":"EM","start":67,"end":71,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_41.markups.3":{"type":"EM","start":451,"end":453,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_42":{"id":"1db99ddd69b6_42","name":"da9d","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def replay(self):\n        batch_size = 32\n        if len(self.memory) \u003C batch_size: \n            return","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_43":{"id":"1db99ddd69b6_43","name":"4f3c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"        samples = random.sample(self.memory, batch_size)\n        for sample in samples:\n            state, action, reward, new_state, done = sample\n            target = self.target_model.predict(state)\n            if done:\n                target[0][action] = reward\n            else:\n                Q_future = max(\n                    self.target_model.predict(new_state)[0])\n                target[0][action] = reward + Q_future * self.gamma\n            self.model.fit(state, target, epochs=1, verbose=0)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_44":{"id":"1db99ddd69b6_44","name":"43b3","type":"P","href":null,"layout":null,"metadata":null,"text":"And finally, we have to reorient our goals, where we simply copy over the weights from the main model into the target one. Unlike the main train method, however, this target update is called less frequently:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_45":{"id":"1db99ddd69b6_45","name":"dfaf","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def target_train(self):\n        weights = self.model.get_weights()\n        target_weights = self.target_model.get_weights()\n        for i in range(len(target_weights)):\n            target_weights[i] = weights[i]\n        self.target_model.set_weights(target_weights)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_46":{"id":"1db99ddd69b6_46","name":"5b30","type":"P","href":null,"layout":null,"metadata":null,"text":"DQN Action","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_46.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_46.markups.0":{"type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_47":{"id":"1db99ddd69b6_47","name":"6310","type":"P","href":null,"layout":null,"metadata":null,"text":"The final step is simply getting the DQN to actually perform the desired action, which alternates based on the given epsilon parameter between taking a random action and one predicated on past training, as follows:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_48":{"id":"1db99ddd69b6_48","name":"3eab","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def act(self, state):\n        self.epsilon *= self.epsilon_decay\n        self.epsilon = max(self.epsilon_min, self.epsilon)\n        if np.random.random() \u003C self.epsilon:\n            return self.env.action_space.sample()\n        return np.argmax(self.model.predict(state)[0])","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_49":{"id":"1db99ddd69b6_49","name":"ca32","type":"P","href":null,"layout":null,"metadata":null,"text":"Training Agent","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_49.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_49.markups.0":{"type":"STRONG","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_50":{"id":"1db99ddd69b6_50","name":"1973","type":"P","href":null,"layout":null,"metadata":null,"text":"Training the agent now follows naturally from the complex agent we developed. We have to instantiate it, feed it the experiences as we encounter them, train the agent, and update the target network:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_51":{"id":"1db99ddd69b6_51","name":"8934","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def main():\n    env     = gym.make(\"MountainCar-v0\")\n    gamma   = 0.9\n    epsilon = .95","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_52":{"id":"1db99ddd69b6_52","name":"0660","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    trials  = 100\n    trial_len = 500","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_53":{"id":"1db99ddd69b6_53","name":"2de3","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    updateTargetNetwork = 1000\n    dqn_agent = DQN(env=env)\n    steps = []\n    for trial in range(trials):\n        cur_state = env.reset().reshape(1,2)\n        for step in range(trial_len):\n            action = dqn_agent.act(cur_state)\n            env.render()\n            new_state, reward, done, _ = env.step(action)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_54":{"id":"1db99ddd69b6_54","name":"5989","type":"PRE","href":null,"layout":null,"metadata":null,"text":"            reward = reward if not done else -20\n            print(reward)\n            new_state = new_state.reshape(1,2)\n            dqn_agent.remember(cur_state, action, \n                reward, new_state, done)\n            \n            dqn_agent.replay()\n            dqn_agent.target_train()","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_55":{"id":"1db99ddd69b6_55","name":"0eee","type":"PRE","href":null,"layout":null,"metadata":null,"text":"            cur_state = new_state\n            if done:\n                break\n        if step \u003E= 199:\n            print(\"Failed to complete trial\")\n        else:\n            print(\"Completed in {} trials\".format(trial))\n            break","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_56":{"id":"1db99ddd69b6_56","name":"9efa","type":"P","href":null,"layout":null,"metadata":null,"text":"Complete Code","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:1db99ddd69b6_56.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_56.markups.0":{"type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:1db99ddd69b6_57":{"id":"1db99ddd69b6_57","name":"94b5","type":"P","href":null,"layout":null,"metadata":null,"text":"With that, here is the complete code used for training against the “MountainCar-v0” environment using DQN!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_58":{"id":"1db99ddd69b6_58","name":"19ef","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:1db99ddd69b6_58.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:d989aded95a0d7dd02464fc56ddbb54e":{"id":"d989aded95a0d7dd02464fc56ddbb54e","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"","__typename":"MediaResource"},"$Paragraph:1db99ddd69b6_58.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:d989aded95a0d7dd02464fc56ddbb54e","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:1db99ddd69b6_59":{"id":"1db99ddd69b6_59","name":"fd10","type":"P","href":null,"layout":null,"metadata":null,"text":"Keep an eye out for the next Keras+OpenAI tutorial!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:1db99ddd69b6_60":{"id":"1db99ddd69b6_60","name":"0ff4","type":"H3","href":null,"layout":null,"metadata":null,"text":"Comment and click that ❤️ below to show support!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:keras":{"id":"keras","displayTitle":"Keras","__typename":"Tag"},"Tag:deep-leraning":{"id":"deep-leraning","displayTitle":"Deep Leraning","__typename":"Tag"},"Tag:reinforcement-learning":{"id":"reinforcement-learning","displayTitle":"Reinforcement Learning","__typename":"Tag"},"Tag:towards-data-science":{"id":"towards-data-science","displayTitle":"Towards Data Science","__typename":"Tag"},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"$Post:1eed3a5338c.previewContent":{"subtitle":"Quick Recap","__typename":"PreviewContent"}}</script><script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/manifest.b2f5c81d.js"></script><script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/vendors_main.a133fb80.chunk.js"></script><script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/main.b6ab3697.chunk.js"></script><script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/vendors_screen.collection.packageBuilder_screen.landingpages.pres45_screen.landingpages.tribute_scre_3e410f11.d2f0cb5c.chunk.js"></script>
<script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/screen.collection.packageBuilder_screen.landingpages.pres45_screen.landingpages.tribute_screen.post__4767c889.764d541b.chunk.js"></script>
<script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/screen.collection.packageBuilder_screen.landingpages.pres45_screen.landingpages.tribute_screen.post__73c4bb05.61e342f3.chunk.js"></script>
<script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/screen.post_screen.post.amp_screen.profile_screen.sequence.library_screen.sequence.post_screen.stori_aba94ffa.5a17883b.chunk.js"></script>
<script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/screen.post.9ea32b4e.chunk.js"></script><script>window.main();</script><script src="./Reinforcement Learning w_ Keras + OpenAI_ DQNs - Towards Data Science_files/p.js" async="" id="parsely-cf"></script><div><div class="s t jz ow ak ox oy oz pa pb pc pd pe pf pg ph pi pj pk pl pm pn po pp pq pr ps pt pu pv pw px py pz qa qb"><div><div class="branch-journeys-top"><div class="n ey"><p class="nc qc as b at au av aw r">To make Medium work, we log user data. By using Medium, you agree to our <a href="https://medium.com/policy/f03bf92035c9" class="dc dd bb bc bd be bf bg bh bi bl bm fp fq oq" target="_blank" rel="noopener">Privacy Policy</a>, including cookie policy.</p><div class="qd r qe"><div class="bh r hs v"><span class="as b at au av aw r ax ay"><button class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" data-testid="close-button"><svg width="19" height="19" viewBox="0 0 19 19"><path d="M13.8 4.6L9.5 8.89 5.21 4.6l-.61.61 4.29 4.3-4.29 4.28.61.62 4.3-4.3 4.28 4.3.62-.62-4.3-4.29 4.3-4.29" fill-rule="evenodd"></path></svg></button></span></div></div></div></div></div></div></div></body></html>